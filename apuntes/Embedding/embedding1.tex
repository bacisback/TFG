\documentclass[8pt,a4paper]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{setspace}
\onehalfspace
\providecommand{\abs}[1]{\lvert#1\rvert}
\providecommand{\norm}[1]{\lVert#1\rVert}
\begin{document}
\title{Reproducing Kernel Hilbert Spaces(RKHS)}
\author{Roberto Alcover Couso}
\date{29/9/2018}
\begin{titlepage}
\maketitle
\end{titlepage}
\section{Introduction}
In this document we will study a special type of Hilbert spaces, RKHS, with a kernel which meets the reproducing property. Understanding this is key for our study due to it's relevance in statistical models and the ideas behind algorithims such as RDC,HSIC...
Furthermore we will define a homogeneity test based on embeddings of probability distributions on RKHSs, where the distance between distributions corresponds to the distance between their embeddings. We will see that the unit ball of an RKHS is a rich enough space so that the expression for the discrepancy vanishes only if the two probability distributions are equal. At the same time it is restrictive enough for the empirical estimate at the discrepancy to converge quickly to its population counterpart as the sample size increases.
\subsection{Preliminar knowledge}

\begin{enumerate}
\item \textsf{Feature map}: $\phi$ is known as a feature map if is a function which maps the data to a Hilbert space $\mathcal{H}$ (feature space).

\begin{align*}
\varphi : \mathcal{X} \rightarrow \mathcal{H} \\
x \mapsto \varphi
\end{align*}
\item \textsf{Kernel function}:
k is called a kernel function if it is the dot product defined on a feature space.

Then, we can rewrite the dot product of the space in terms of this mapping:

\begin{align*}
k: \mathcal{X}\times\mathcal{X} \rightarrow \mathcal{H} \\
(x,x') \mapsto k(x,x')=<\phi(x),\phi(x')>
\end{align*}
\item \textsf{Reproducing kernel}:
a function k is a reproducing kernel of the Hilbert space $\mathcal{H}$ is and only if it satisfies:
\begin{enumerate}
\item k(x,.)$\in \mathcal{H}, \forall x \in \mathcal{X}$
\item \textit{Reproducing property}: $<f,k(x,\cdot)> = f(x) \forall f \in \mathcal{H} , \forall x \in \mathcal{X}$
\end{enumerate}
\end{enumerate}
\paragraph*{Proposition 1.}
If k is a reproducting kernel then: k(x,x') = <k(x,.),k(x',.)>
\section{Maximum mean discrepancy}
In this section it'll be shown how RKHSs can be used to define a homogeneity test in terms of the embeddings of the probability measures.
This test consist in maximizing the measure of discrepancy between functions that belong to a certain family $\mathcal{F}$ which must be rich enough to detect all the possible differences between the two probability measures.
\subsection{Mean embedding}
\textsf{Lemma 1.} Given two Borel probability measures $\mathbb{P}$ and $\mathbb{Q}$  are equal if and only if $\mathbb{E}f(X) = \mathbb{E}f(Y)$  $\forall f \in \mathcal{C(X)}$

$$X \sim \mathbb{P} \text{ and } Y \sim \mathbb{Q}$$
\begin{flushleft}
This condition is pretty dificult to prove therefore we will keep our study in order to simplify this evaluation.
\end{flushleft}

\textsf{\textbf{Definition 1. MMD}}
\begin{flushleft}

Let $\mathcal{F}$ be a class of functions f: $X \rightarrow \mathbb{R}$ the MMD based on $\mathcal{F}$ is
\end{flushleft}

\begin{center}
$\gamma(\mathbb{P},\mathbb{Q}) = MMD(\mathcal{F},\mathbb{P},\mathbb{Q}) =  \sup\limits_{f\in\mathcal{F}}\{\mathbb{E}f(X) -\mathbb{E}f(Y)\}$
\end{center}
\begin{flushleft}
This $\mathcal{F}$ must be rich enough for it to ensure that $\mathbb{P} = \mathbb{Q} \leftrightarrow \gamma(\mathbb{P},\mathbb{Q}) = 0$. And restrictive enough for the empirical estimate to converge quickly as the sample size increases.
This will be done through RKHS with a characteristic kernel K
\end{flushleft}

\textsf{\textbf{Riesz representation}}
\begin{flushleft}
If T is a bounded linear operator on a Hilbert space $\mathcal{H}$, then there exist some $g \in \mathcal{H}$ such that $\forall f \in \mathcal{H}$:
\end{flushleft}
\begin{center}
$T(f) = <f,g>_{\mathcal{H}}$
\end{center}

\textsf{\textbf{Lemma 2.}}
\begin{flushleft}
Given a K(s,) semi positive definite, measurable and $\mathbb{E}\sqrt{k(X,X)}<\infty$, where X$\sim \mathbb{P}$ then $\mu_{p} \in \mathcal{H}$ exist and fulfulls the next condition
$\mathbb{E}f(X) = <f,\mu_{p}>$ for all f $\in \mathcal{H}$
\end{flushleft}

\textit{Proof} Lets define the linear operator $T_{\mathbb{P}}f\equiv\mathbb{E}(\sqrt{k(X,X)}) < \infty \forall f\in \mathcal{H}$ 

$$|T_{\mathbb{P}}f| = |\mathbb{E}(f(X))| \leq \mathbb{E}(|f(X)|) =\footnote{Reproducing property of the kernel} \mathbb{E}\abs{<f,k(\cdot,X)>_{\mathcal{H}}} \leq\footnote{Chauchy Schwarz inequality} \norm{f} _{\mathcal{H}}\cdot\mathbb{E}(\sqrt{K(X,X)})^{1/2} <\footnote{The expectation under $\mathbb{P}$ of the kernel is bounded} \infty $$



Then using the Riesz representation theorem applied to $T_{p}$, there exist a $\mu_{p}\in \mathcal{H}$ such that $T_{p}f = <f,\mu_{p}>_{\mathcal{H}}$


\textsf{\textbf{Definition 2. Mean embedding}}

Given a probability distribution $\mathbb{P}$ we will define the mean embedding of $\mathbb{P}$ as an element $\mu_{p} \in \mathcal{H}$ such that

$$\mathbb{E}(f(X))=<f,\mu_{\mathbb{P}}>_{\mathcal{H}}, \forall f \in \mathcal{H}$$

If $f\in \mathcal{H}$ and $\mu_{\mathbb{P}} \in \mathbb{R}$
$\mathbb{E}(f(X)) = \lim\limits_{N \to \infty} \frac{1}{N} \sum\limits_{n=1}^{N}f(x_{n})$

Applying the Riesz representation theorem to represent $f(x_{n})$

$\forall x_{n}$ then:

$$f(x_{n}) = <f,K(\cdot,x_{n})>_{\mathcal{H}}$$


then

$$\lim\limits_{N \to \infty} \frac{1}{N} \sum\limits_{n=1}^{N}f(x_{n}) = \lim\limits_{N \to \infty} \frac{1}{N} \sum\limits_{n=1}^{N} <f,K(\cdot,x_{n}>_{\mathcal{H}} = <f, \lim\limits_{N \to \infty} \frac{1}{N} \sum\limits_{n=1}^{N}K(\cdot,x_{n})>_{\mathcal{H}}$$

which leads to the final conclussion:

$\mu_{\mathbb{P}} \equiv \mathbb{E}_{X\sim \mathbb{P}}(K(t,X))$  $t \in [0,T]$

\textsc{Second interpretation of the mean embedding}

$$\mu_{\mathbb{P}} = \mathbb{E}(K(\cdot,X))$$


\subsection{Introduction to MMD}
\textsf{Lemma 3.} Given the conditions of Lemma 2 ($\mu_{\mathbb{P}} \text{and} \mu_{\mathbb{Q}}$ exist) then:

$X \sim \mathbb{P} \mu_{\mathbb{P}} \equiv \mathbb{E}_{X\sim \mathbb{P}}(K(\cdot,X))$ $Y \sim \mathbb{Q} \mu_{\mathbb{Q}} \equiv \mathbb{E}_{Y\sim \mathbb{Q}}(K(\cdot,Y))$

and:

$$MMD(\mathcal{F},\mathbb{P}, \mathbb{Q}) = \norm{\mu_{\mathbb{P}} - \mu_{\mathbb{Q}}}_{\mathcal{H}}$$

\textit{Proof}

$$MMD \equiv \sup\limits_{f\in \mathcal{H} \\\ \norm{f} \leq 1}\{\mathbb{E}(f(x)) - \mathbb{E}(f(y))\}$$
$$=\sup\limits_{f\in \mathcal{H} \\\ \norm{f} \leq 1}\{<f,\mu_\mathbb{P}> - <f, \mu_\mathbb{Q}> \} $$
$$=\sup\limits_{f\in \mathcal{H} \\\ \norm{f} \leq 1} <f,(\mu_{\mathbb{P}} - \mu_{\mathbb{Q}})> $$
$$\leq\footnote{ Cauchy Schwarz inequality} \sup\limits_{f\in \mathcal{H} \\\ \norm{f} \leq 1}\{\norm{f}_{\mathcal{H}},\norm{\mu_\mathbb{P} - \mu_{\mathbb{Q}}}_{\mathcal{H}}\}$$
$$\leq \norm{\mu_{\mathbb{P}} - \mu_{\mathbb{Q}}}_{\mathcal{H}}$$



But on the other side, if we choose f as:

$$f=\frac{1}{\norm{\mu_{\mathbb{P}}- \mu_{\mathbb{Q}}}} (\mu_{\mathbb{P}}- \mu_{\mathbb{Q}})$$

then we have:

$$\sup\limits_{f\in \mathcal{H} \\\ \norm{f} \leq 1}\{\norm{f}_{\mathcal{H}},\norm{\mu_\mathbb{P} - \mu_{\mathbb{Q}}}_{\mathcal{H}}\} \geq \norm{\mu_{\mathbb{P}}- \mu_{\mathbb{Q}}}_{\mathcal{H}} $$

therefore

$$MMD = \norm{\mu_{\mathbb{P}}- \mu_{\mathbb{Q}}}_{\mathcal{H}}$$

\textsf{\textbf{Proposition 1}}

Given:
$X,X' \sim \mathbb{P} \text{ and } Y,Y' \sim \mathbb{Q}$ and X and Y are independent then:
$$MMD^{2}(\mathcal{F},\mathbb{P},\mathbb{Q}) = \mathbb{E}(K(X,X')) + \mathbb{E}(K(Y,Y')) - 2\mathbb{E}K(X,Y).$$

\textit{proof}

$$MMD^{2}(\mathcal{F},\mathbb{P},\mathbb{Q}) = \norm{\mu_{\mathbb{P}}- \mu_{\mathbb{Q}}}_{\mathcal{H}}^{2}$$
$$=<\mu_{\mathbb{P}}- \mu_{\mathbb{Q}},\mu_{\mathbb{P}}- \mu_{\mathbb{Q}}>_{\mathcal{H}}$$
$$=<\mathbb{E}(K(\cdot,X))-K(\cdot,Y)),\mathbb{E}(K(\cdot,X'))-K(\cdot,Y'))>$$
$$=\mathbb{E}(<K(\cdot,X),K(\cdot,X')> + <K(\cdot,Y),K(\cdot,Y')> - 2<K(\cdot,X)K(\cdot,Y)>)$$
$$=\footnote{is due to the reproductive property of the kernel.} \mathbb{E}(K(X,X') + K(Y,Y') -2K(X,Y))
$$
$$= \mathbb{E}(K(X,X')) + \mathbb{E}(K(Y,Y')) -2\mathbb{E}(K(X,Y))
$$
$$= \int\int K(s,t) \underbrace{d(\mathbb{P}-\mathbb{Q})(s)}_{Signed measure} d(\mathbb{P}-\mathbb{Q})(t)
$$

\subsection*{Prooving that MMD defines an homogeneity test}

\textsf{\textbf{Definition 3. Characteristic kernel}}

A reproducing kernel k is a characterisctic kernel if the induced $\gamma_{k}$ is a metric.

\textsf{\textbf{Theorem 2.}}

If X is a compact metric space, k is continuous and $\mathcal{H}$ is dense in $\mathcal{C}$(X) with respect to the supremum norm, then $\mathcal{H}$ is characteristic.

\textit{Proof.}
Being characteristic means that 
$MMD(\mathcal{F},\mathbb{P},\mathbb{Q}) = 0 \leftrightarrow \mathbb{P} = \mathbb{Q}$
\begin{flushleft}
$\rightarrow$
\end{flushleft}

By lemma 1 we know that $\mathbb{P}$ and $\mathbb{Q}$  are equal if and only if $\mathbb{E}f(X) = \mathbb{E}f(Y)$  $\forall f \in \mathcal{C(X)}$

Given that $\mathcal{H}$ is dense in $\mathcal{C}$(X) then:
$$\forall \epsilon >0, f\in\mathcal{C}(X), \exists g\in \mathcal{H} : \norm{f-g}_{\infty} < \epsilon$$

\begin{align*}
\abs{\mathbb{E}(f(X)) - \mathbb{E}(f(Y))}  = \abs{\mathbb{E}(f(X)) - \mathbb{E}(g(X)) + \mathbb{E}(g(X)) - \mathbb{E}(g(Y)) +\mathbb{E}(g(Y)) - \mathbb{E}(f(Y))} \\
\leq \abs{\mathbb{E}(f(X)) - \mathbb{E}(g(X))} + \abs{\mathbb{E}(g(X)) - \mathbb{E}(g(Y))} +\abs{\mathbb{E}(g(Y)) - \mathbb{E}(f(Y))}\\
= \abs{\mathbb{E}(f(X)) - \mathbb{E}(g(X))} + \abs{<g,\mu_{\mathbb{P}}-\mu_{\mathbb{Q}}>_{\mathcal{H}} } +\abs{\mathbb{E}(g(Y)) - \mathbb{E}(f(Y))} \\
\leq \mathbb{E}\abs{f(X) - g(X)} + \abs{<g,\mu_{\mathbb{P}}-\mu_{\mathbb{Q}}>_{\mathcal{H}} } +\mathbb{E}\abs{g(Y) - f(Y)} \\ \leq^{1} \norm{f-g}_{\infty}  + \abs{<g,\mu_{\mathbb{P}}-\mu_{\mathbb{Q}}>_{\mathcal{H}} } + \norm{f-g}_{\infty} 
\\
\leq \abs{<g,\mu_{\mathbb{P}}-\mu_{\mathbb{Q}}>_{\mathcal{H}} } + 2\epsilon
\end{align*}

By lemma 3 we know that if MMD = 0 then $\mu_{\mathbb{P}} = \mu_{\mathbb{Q}}$. Hence:
$$\abs{\mathbb{E}(f(X)) - \mathbb{E}(f(Y))} \leq 2\epsilon$$

Then by lemma 1 $\mathbb{P}$ and $\mathbb{Q}$  are equal.

\begin{flushleft}
$\leftarrow$
\end{flushleft}
By definition of MMD.

\subsection{Application to independence test}

From the MMD criterion we will develop an independence criterion which will be conduced by the following idea:
Given $\mathcal{X} \sim \mathbb{P}$ and $\mathcal{Y} \sim \mathbb{Q}$ whose joint distribution is $\mathbb{P}_{\mathcal{XY}}$ then the test of independence between these variables will be determining if $\mathbb{P}_{\mathcal{XY}}$ is equal to the product of the marginals $\mathbb{P}\mathbb{Q}$. Therefore:

$\mathcal{MMD}(\mathcal{F}, \mathbb{P}_{\mathcal{XY}},\mathbb{P}\mathbb{Q}) = 0$ if and only if $\mathcal{X}$ and $\mathcal{Y}$ are independent.
To characterize this independence test we need to introduce a new RKHS, which is a tensor product of the RKHS’s in which the marginal distributions of the random variables are embedded. Let $\mathcal{X}$ and $\mathcal{Y}$ be two topological spaces and let k and l be kernels on these spaces, with respective RKHS $\mathcal{H}$ and $\mathcal{G}$. Let us denote as $\upsilon((x, y), (x' , y ' ))$ a kernel on the product space $\mathcal{X}\times\mathcal{Y}$ with RKHS $\mathcal{H}_{\upsilon}$. This space is known as the tensor product space $\mathcal{H}\times\mathcal{G}$. Tensor product spaces are defined as follows:

\textsf{\textbf{Definition 4. Tensor product}}
The tensor product of Hilbert spaces$\mathcal{H}_{1}$ and $\mathcal{H}_{2}$  with inner products $<·, ·>_{1}$ and
$<·, ·>_{2}$ is defined as the completion of the space $\mathcal{H}_{1}\times\mathcal{H}_{2}$  with inner product  $<·, ·>_{1}$ $<·, ·>_{2}$extended
by linearity. The resulting space is also a Hilbert space.

\textsf{Lemma 4.} 

A kernel $\upsilon$ in the tensor product space  $\mathcal{H}\times\mathcal{G}$ can be defined as:
$$\upsilon((x,y),(x',y')) = k(x,x')l(y,y')$$

\paragraph{Useful definitions for the following content}
$$\mathbb{E}_{\mathcal{X}}f(\mathcal{X}) = \int f(x)d\mathbb{P}(x)$$
$$\mathbb{E}_{\mathcal{Y}}f(\mathcal{Y}) = \int f(y)d\mathbb{Q}(y)$$
$$\mathbb{E}_{\mathcal{X}\mathcal{Y}}f(\mathcal{X}\mathcal{Y}) = \int f(x,y)d\mathbb{P}_{\mathcal{X}\mathcal{Y}}(x,y)$$

Using this notation, the mean embedding of $\mathbb{P}_{\mathcal{X}\mathcal{Y}}$ and $\mathbb{P}\mathbb{Q}$ are:
$$\mu_{\mathbb{P}_{\mathcal{X}\mathcal{Y}}} = \mathbb{E}_{\mathcal{X}\mathcal{Y}}\upsilon((\mathcal{X},\mathcal{Y}),)$$
$$\mu_{\mathbb{P}\mathbb{Q}} = \mathbb{E}_{\mathcal{X}\mathcal{Y}}\upsilon((\mathcal{X},\mathcal{Y}),)$$
In terms of these embeddings:
$$\mathcal{MMD}(\mathcal{F}, \mathbb{P}_{\mathcal{XY}},\mathbb{P}\mathbb{Q}) = \norm{\mathbb{P}_{\mathcal{X}\mathcal{Y}}-\mu_{\mathbb{P}\mathbb{Q}} }_{\mathbb{H}_{\upsilon}}$$

\section{HSIC}

\end{document}