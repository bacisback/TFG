As we did for the previous sections we will an homogeneity test based on energy distance, then we will use it to formulate another independence test, distance covariance and distance correlation. This test is one of the most popular nowadays because of its power and the fact that it does not depend on any parameter. Most of the content of this section is taken from \cite{DCOV_1} and \cite{BrownianCovariance}

\subsection{Definitions}
In this subsection we will introduce some Definitions which will be used to build the independence test, this test is a generalization of the characteristic function of each distribution.

\begin{prop}
Let $\mathcal{F}$ and $\mathcal{G}$ be two CDFs of the independent random variables X,Y respectively and  X',Y' two iid copies of them, then:
\vspace{5mm}
$$
2\int_{-\infty}^{\infty}(\mathcal{F}(x) - \mathcal{G}(x))^{2} dx = 2\mathbb{E}\abs{X-Y} - \mathbb{E}\abs{X-X'} -\mathbb{E}\abs{Y-Y'}  
$$
\end{prop}

In the Appendix \ref{dems} Section \ref{P:ED} is shown the proof of this proposition.

\begin{defn}
Let X and Y be random variables in $\mathbb{R}^{d}$ of $\mathbb{E}\norm{X}_{d} + \mathbb{E}\norm{Y}_{d} < \infty$ the energy distance between X and Y is defined as:

\begin{equation}{}
\varepsilon (X,Y) = 2\mathbb{E}\norm{X-Y}_{d} - \mathbb{E}\norm{X-X'}_{d} - \mathbb{E}\norm{Y-Y'}_{d} 
\end{equation}\label{energy_distance}
where X' and Y' are i.i.d copies of X and Y respectively.
The energy distance can also be defined in terms of the characteristic functions. In fact, it can be seen as a weighted $\mathcal{L}_{2}$ distance between characteristic functions.
\end{defn}
\begin{prop}
Given two independent d-dimensional random variables X and Y, with distributions $\mathbb{P}$ and $\mathbb{Q}$ respectively such that $\mathbb{E}\norm{X}_{d} + \mathbb{E}\norm{Y}_{d} < \infty$ he energy distance between X and Y can be written as:
\vspace{5mm}
$$\varepsilon(X,Y) = \frac{1}{c_{d}}\int_{\mathbb{R}^{d}}\frac{\abs{\phi_{\mathbb{P}}(t)-\phi_{\mathbb{Q}}(t)}^{2}}{\norm{t}^{d+1}_{d}}$$
where
$$c_{d} = \frac{\pi^{\frac{d+1}{2}}}{\Gamma(\frac{d+1}{2})}$$
being $\Gamma(\cdot)$ the gamma funcion
\end{prop} \label{DcovPropRara}
In the Appendix \ref{dems} Section \ref{P:ED} is shown the proof of this proposition.
 
It is easy to see that the energy distance only vanishes when the distributions are equal, since it is equivalent to having equal characteristic functions.
\subsection{Application to an independence test}
In this subsection we will use the knowledge aquired above to develop a new independence test. This new test is called distance covariance (DCOV), it's name comes from the fact that it is a generalization of the classical product-moment covariance.

We will start by defining the independence test. Given the random vectors $X\in\mathbb{R}^{dx},Y\in\mathbb{R}^{dy}$, distributions $\mathbb{P}_{X} and \mathbb{P}_{Y}$ respectively.Let $\phi_{\mathbb{P}_{X}}, \phi_{\mathbb{P}_{Y}}$ denote their characteristic functions and $\phi_{\mathbb{P}_{XY}}$ the characteristic function of the joint distribution. X and Y are independent if and only if 
$\phi_{\mathbb{P}_{X}}\phi_{\mathbb{P}_{Y}} = \phi_{\mathbb{P}_{XY}}$. The covariance energy test is based on measuring a distance between these functions.

First we need to generalize the energy distance expression for random vectors of different dimensions. As defined earlier this expression is obtained from a weighted $mathcal{L}_{2}$-distance, imposing rotation invariance and scale equivariance, the energy distance is:
\vspace{5mm}

$$\varepsilon(X,Y) = \frac{1}{c_{d_{x}}c_{d_{y}}}\int_{\mathbb{R}^{d_{x} + d_{y}}}\frac{\abs{\phi_{\mathbb{P}}(t)-\phi_{\mathbb{Q}}(t)}^{2}}{\norm{t}^{d_{x}+1}_{d_{x}}\norm{s}^{d_{y}+1}_{d_{y}}}dtds$$

Where $c_{d}$ is defined as before.
The distance covariance is defined by replacing $\phi_{\mathbb{P}}$ and $\phi_{\mathbb{Q}}$ in the previous formula with characteristic functions of the joint distribution and the product of the marginals respectively.
\begin{defn}
The distance covatiance, DCOV, between random vectors X and Y, with $\mathbb{E}\norm{X}_{d_{x}} + \mathbb{E}\norm{Y}_{d_{y}} < \infty$, is the nonnegative number $\nu^{2}(X,Y)$ defined by:
\vspace{5mm}
$$
\nu^{2}(X,Y) = \norm{\phi_{\mathbb{P}_{X,Y}}(t,s)-\phi_{\mathbb{P}_{X}}(t)\phi_{\mathbb{P}_{Y}}(s)}^{2}_{w} = \frac{1}{c_{d_{x}}c_{d_{y}}}\int_{\mathbb{R}^{d_{x} + d_{y}}}\frac{\abs{\phi_{\mathbb{P}_{X,Y}}(t,s)-\phi_{\mathbb{P}_{X}}(t)\phi_{\mathbb{P}_{Y}}(s)}^{2}}{\norm{t}^{d_{x}+1}_{d_{x}}\norm{s}^{d_{y}+1}_{d_{y}}} dtds
$$
\end{defn}
\begin{defn}
The distance correlation, DCOR, between random vectors X and Y, ith $\mathbb{E}\norm{X}_{d_{x}} + \mathbb{E}\norm{Y}_{d_{y}} < \infty$, is the nonnegative number $\mathcal{R}(X,Y)$ defined by:

$$
\mathcal{R}(X,Y) = \left \{
	\begin{array}{c} 
		\frac{\nu^{2}(X,Y)}{\sqrt{\nu^{2}(X)\nu^{2}(Y)}} \text{ if } \nu^{2}(X)\nu^{2}(Y) > 0 \\ 
		\\
		0 \text{ if } \nu^{2}(X)\nu^{2}(Y) = 0 
	\end{array}
	\right.  
$$
\end{defn}
The distance covariance, like the energy distance, can be expressed using expectations.
\begin{lem}
Let (X,Y),(X',Y'),(X'',Y'') $\sim \mathbb{P}_{XY}$ be iid copies of (X,Y), it holds that:
\begin{equation}{DCOV}
\begin{split}
\nu^{2}(X,Y)  
&= \mathbb{E}_{XY}\mathbb{E}_{X'Y'}\norm{X-X'}_{d_{x}}\norm{Y-Y'}_{d_{y}} + \mathbb{E}_{X}\mathbb{E}_{X'}\norm{X-X'}_{d_{x}}\mathbb{E}_{Y}\mathbb{E}_{Y'}\norm{Y-Y'}_{d_{y}} \\
&-2\mathbb{E}_{XY}[\mathbb{E}_{X'}\norm{X-X'}_{d_{x}}\mathbb{E}_{Y'}\norm{Y-Y'}_{d_{y}}]
\end{split}
\end{equation} 
This proof is similar to the one of \ref{DcovDemRara}. therefore we will leave it for the interested readers.
\end{lem}

\subsection{Statistics}
Now we will give some estimators for both energy distance and distance covariance. Since we are interested in testing independence we will focus on the DCOV estimator. We will start with an estimator of energy distance, which as it's explained above it's a homogeneity test.
Given the definition of energy distance \ref{energy_distance} now we will define it's statistic as: Given two independent random samples $x = (x_{1},...,x_{n})$ and  $y = (y_{1},...,y_{m})$, the two sample energy statistic corresponding to $\varepsilon(X,Y)$ is:

$$
\varepsilon_{n,m}(x,y) = \frac{2}{nm}\sum_{i=1}^{n}\sum_{j=1}^{m}\norm{x_{i}-y_{j}} - \frac{1}{n^{2}}\sum_{i=1}^{n}\sum_{j=1}^{n}\norm{x_{i}-x_{j}}- \frac{1}{m^{2}}\sum_{i=1}^{m}\sum_{j=1}^{m}\norm{y_{i}-y_{j}}
$$
Finally, an estimator of the distance covariance can be obtained directly from \ref{DCOV}.For a random sample $(x,y) = ((x_{1},y_{1}),...,(x_{n},y_{n}))$ of iid random vectors generated from the joint distribution of $X \in \mathbb{R}^{dx}$ and $Y \in \mathbb{R}^{dy}$, we obtain:
\begin{equation}{}
\begin{split}
\nu^{2}(X,Y)  
&= \frac{1}{n^{2}}\sum_{i,j=1}^{n}\norm{x_{i}-x_{j}}_{d_{x}}\norm{y_{i}-y_{j}}_{d_{y}} + \frac{1}{n^{2}}\sum_{i,j=1}^{n}\norm{x_{i}-x_{j}}_{d_{x}}\frac{1}{n^{2}}\sum_{i,j=1}^{n}\norm{y_{i}-y_{j}}_{d_{y}}\\
& -\frac{2}{n^{3}}\sum_{i=1}^{n}[\sum_{j=1}^{n}\norm{x_{i}-x_{j}}_{d_{x}}\sum_{j=1}^{n}\norm{y_{i}-y_{j}}_{d_{y}}]
\end{split}
\end{equation} 

As we can see this estimate cost is $O(n^{2})$, that's the reason we won't calculate the distance covariance this way, our new approach will go as follows:
First we compute the Euclidean distance matrix of each sample, computing all the pairwise distances between sample observations:

$$(a_{ij}) = (\norm{x_{i}-x_{j}}_{dx}), (b_{ij}) = (\norm{y_{i}-y_{j}}_{dy}).$$

an easy way to compute this matrix is:

$$A_{ij} = a_{ij} + \overline{a_{i \cdot}} - \overline{a_{\cdot j}} + \overline{a}, for i,j = 1,...,n$$

where:

$$\overline{a_{i\cdot}} = \frac{1}{n}\sum_{k=1}^{n}a_{ik},\overline{a_{\cdot j}} = \frac{1}{n}\sum_{k=1}^{n}a_{kj}, \overline{a} =  \frac{1}{n^{2}}\sum_{k=1}^{n}\sum_{l=1}^{n}a_{lk}$$

equivalently for B. In terms of these matrix, the distance covariance $\nu^{2}(x,y)$ is:

$$\nu^{2}(x,y) = \frac{1}{n^{2}}\sum_{i=1}^{n}\sum_{j=1}^{n}A_{ij}B_{ij}$$
Finally the distance correlation is:

$$
\mathcal{R}(X,Y) = \left \{
	\begin{array}{c} 
		\frac{\nu^{2}_{n}(x,y)}{\sqrt{\nu^{2}_{n}(x)\nu^{2}_{n}(y)}} \text{ if } \nu^{2}_{n}(x)\nu^{2}_{n}(y) > 0 \\ 
		\\
		0 \text{ if } \nu^{2}_{n}(x)\nu^{2}_{n}(y) = 0 
	\end{array}
	\right.  
$$
where: 

$$
\nu^{2}_{n}(x) = \nu^{2}_{n}(x,x) = \frac{1}{n^{2}}\sum_{i=1}^{n}\sum_{j=1}^{n}A_{ij}
$$
$$
\nu^{2}_{n}(y) = \nu^{2}_{n}(y,y) = \frac{1}{n^{2}}\sum_{i=1}^{n}\sum_{j=1}^{n}B_{ij}
$$

Now we need to prove that this statistics converge almost surely when the random vectors have finite first moments which is: 
if $\mathbb{E}\norm{X} + \mathbb{E}\norm{Y} < \infty$ then

$$
\lim_{n\to\infty} \nu^{2}_{n}(x,y) \xrightarrow{a.s} \nu^{2}(X,Y)
$$
In Appendix \ref{dems} Section \ref{P:ED} Subsection \ref{P:DCOVCOV} is proven the convergence.

\paragraph{Asymptotic properties of n$\nu^{2}$}

In \cite{DCOV_1} Section 2.4 \textit{Asymptotic properties of n$\nu^{2}$}, it is proven that if $\mathbb{E}\norm{X+Y}< \infty$ then $\nu^{2}\frac{n}{S2}$ converges in distribution to a quadratic form.
\begin{equation}{Asymptotic quadratic form of DCOV}
Q =^{D}\sum_{j=1}^{\infty} \lambda_{j}Z_{j}^{2}
\end{equation}
where $Z_{j}$ are independent standard normal random variables, a$\lambda_{j}$ are non-negative constants that depend on the distribution og (X,Y), and $E(Q)=1$.

As calculating this distribution is not efficient, in order to create a asymptotic test with a significance level we will use that: the tail of the quadratic form it's similar enough to the one of a Chisquared with one degree of freedom.

$$
\nu^{2}\frac{n}{S2} \geq \mathcal{X}_{1;1-\alpha}^{2}
$$ 

where $\mathcal{X}_{1}^{2}$ denotes a Chisquared distribution with one degree of freedom, and let $0<\alpha\leq0.215$. This result is taken from \cite{DCOV_Chisq}, page 181.





