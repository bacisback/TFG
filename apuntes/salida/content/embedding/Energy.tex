
In this section we will define energy distance and we will use it to define a homogeneity test.
This knowledge will be used in order to formulate another independence test based on energy distance, distance covariance and distance correlation. This test is one of the most popular nowadays because of its power and the fact that it does not depend on any parameter.

\subsection{Definitions}

\begin{prop}
Let $\mathcal{F}$ and $\mathcal{G}$ be two CDFs of the independent random variables X,Y respectively and  X',Y' two iid copies of them, then:
\vspace{5mm}
$$
2\int_{-\infty}^{\infty}(\mathcal{F}(x) - \mathcal{G}(x))^{2} dx = 2\mathbb{E}\abs{X-Y} - \mathbb{E}\abs{X-X'} -\mathbb{E}\abs{Y-Y'}  
$$
\end{prop}
\begin{proof}
We will start analysing the expectations of the right hand side. We will use that for any positive random variable $ Z>0$,
$\mathbb{E}Z = \int_{0}^{\infty} \mathbb{P}(Z>z)dz$
\begin{equation}{}
\begin{split}
\mathbb{E}\abs{X-Y}
& = \int_{0}^{\infty} \mathbb{P}(\abs{X-Y} > u) du \\
&= \int_{0}^{\infty} \mathbb{P}(X - Y > u) du + \int_{0}^{\infty} \mathbb{P}(X - Y < u) du \\
&= \int_{0}^{\infty}\int_{-\infty}^{\infty} \mathbb{P}(X - Y > u|Y = y)d\mathcal{G}(y)du + \int_{0}^{\infty}\int_{-\infty}^{\infty} \mathbb{P}(X - Y < u|X = x)d\mathcal{F}(x)(y)du \\
&=\footnote{due to both having finite expectation we can apply fubini}  
\int_{-\infty}^{\infty}\int_{0}^{\infty} \mathbb{P}(X - Y > u|Y = y)du\mathcal{G}(y) + \int_{-\infty}^{\infty}\int_{0}^{\infty} \mathbb{P}(X - Y < u|X = x)du\mathcal{F}(x) \\
&= \int_{-\infty}^{\infty}\int_{0}^{\infty} \mathbb{P}(X > u + y)du\mathcal{G}(y) + \int_{-\infty}^{\infty}\int_{0}^{\infty} \mathbb{P}(Y > u + x)du\mathcal{F}(x)\\
\end{split}
\end{equation}

Now we use the change of variables z = u + y for the first integral, and w = u + x for the second one. Applying Fubini again:

\begin{equation}{}
\begin{split}
\mathbb{E}\abs{X-Y}
&= \int_{-\infty}^{\infty}\int_{y}^{\infty} \mathbb{P}(X > z)dz\mathcal{G}(y) + \int_{-\infty}^{\infty}\int_{x}^{\infty} \mathbb{P}(Y > w)dw\mathcal{F}(x)\\
&= \int_{-\infty}^{\infty} \mathbb{P}(X > z)dz\int_{y}^{\infty}\mathcal{G}(y) + \int_{-\infty}^{\infty}\mathbb{P}(Y > w)dw\int_{x}^{\infty} \mathcal{F}(x)\\
&= \int_{-\infty}^{\infty} \mathbb{P}(X > z)\mathbb{P}(Y < z)dz + \int_{-\infty}^{\infty}\mathbb{P}(Y > w)\mathbb{P}(X < w)dw\\
&= \int_{-\infty}^{\infty} [(1 - \mathcal{F}(z))\mathcal{G}(z) + (1 - \mathcal{G}(z))\mathcal{F}(z)]dz\\
&= -2\int_{-\infty}^{\infty}\mathcal{F}(z)\mathcal{G}(z)dz + \mathbb{E}\abs{X} + \mathbb{E}\abs{Y}\\
\end{split}
\end{equation}
Taking $\mathcal{F} = \mathcal{G}$ in the previous development:

$$
\mathbb{E}\abs{X-X'} =  -2\int_{-\infty}^{\infty}\mathcal{F}^{2}(z)dz + 2\mathbb{E}\abs{X}
$$

Equivalently for Y. Combining these partial results concludes the proof.

\end{proof}

\begin{defn}
Let X and Y be random variables in $\mathbb{R}^{d}$ of $\mathbb{E}\norm{X}_{d} + \mathbb{E}\norm{Y}_{d} < \infty$ the energy distance between X and Y is defined as:

\begin{equation}{}
\varepsilon (X,Y) = 2\mathbb{E}\norm{X-Y}_{d} - \mathbb{E}\norm{X-X'}_{d} - \mathbb{E}\norm{Y-Y'}_{d} 
\end{equation}\label{energy_distance}
where X' and Y' are i.i.d copies of X and Y respectively.
The energy distance can also be defined in terms of the characteristic functions. In fact, it can be seen as a weighted $\mathcal{L}_{2}$ distance between characteristic functions.
\end{defn}
\begin{prop}
Given two independent d-dimensional random variables X and Y, with distributions $\mathbb{P}$ and $\mathbb{Q}$ respectively such that $\mathbb{E}\norm{X}_{d} + \mathbb{E}\norm{Y}_{d} < \infty$ he energy distance between X and Y can be written as:
\vspace{5mm}
$$\varepsilon(X,Y) = \frac{1}{c_{d}}\int_{\mathbb{R}^{d}}\frac{\abs{\phi_{\mathbb{P}}(t)-\phi_{\mathbb{Q}}(t)}^{2}}{\norm{t}^{d+1}_{d}}$$
where
$$c_{d} = \frac{\pi^{\frac{d+1}{2}}}{\Gamma(\frac{d+1}{2})}$$
being $\Gamma(\cdot)$ the gamma funcion
\end{prop} \label{DcovPropRara}
To prove this proposition we need the following lemma.
\begin{lem}
$\forall x \in \mathbb{R}^{d}$ then:
\vspace{5mm}
$$\int_{\mathbb{R}^{d}}\frac{1-cos(tx)}{\norm{t}^{d+1}_{d}} dt = c_{d}\norm{x}_{d}$$
where tx is the inner product of t and x. \label{cd}
\end{lem}
\begin{proof}
We will begin by applying the following transformation: $z_{1} = \frac{tx}{\norm{x}_{d}}$
followed by the following change of variables:
$s = z\norm{x}_{d}$
\begin{equation}{}
\begin{split}
\int_{\mathbb{R}^{d}}\frac{1-cos(tx)}{\norm{t}^{d+1}_{d}} dt 
&= \int_{\mathbb{R}^{d}}\frac{1-cos(z\norm{x}_{d})}{\norm{z}^{d+1}_{d}} dt \\
&= \int_{\mathbb{R}^{d}}\frac{1-cos(s)}{\frac{\norm{s}_{d}}{\norm{x}_{d}}^{d+1} \norm{x}^{d}_{d}} dt \\
&=\norm{x}_{d}\int_{\mathbb{R}^{d}}\frac{1-cos(s)}{\norm{s}^{d+1}_{d}}ds \\
&=\norm{x}_{d}\frac{\pi^{\frac{d+1}{2}}}{\Gamma(\frac{d+1}{2})}
\end{split}
\end{equation}
\end{proof}
\begin{proof}
\ref{DcovPropRara} Let $\overline{\phi_{\mathbb{P}}(t)}$ denote the complex conjugate of the characteristic function.\label{DcovDemRara}
\begin{equation}{}
\begin{split}
\abs{\phi_{\mathbb{P}}(t)-\phi_{\mathbb{Q}}(t)}^{2} = 
& = (\phi_{\mathbb{P}}(t)-\phi_{\mathbb{Q}}(t))\overline{(\phi_{\mathbb{P}}(t)-\phi_{\mathbb{Q}}(t))} \\
& = (\phi_{\mathbb{P}}(t)-\phi_{\mathbb{Q}}(t))(\overline{\phi_{\mathbb{P}}(t)}-\overline{\phi_{\mathbb{Q}}(t)})\\
&= \phi_{\mathbb{P}}(t)\overline{\phi_{\mathbb{P}}(t)} - \phi_{\mathbb{P}}(t)\overline{\phi_{\mathbb{Q}}(t)} - \phi_{\mathbb{Q}}(t)\overline{\phi_{\mathbb{P}}(t)} + 
\phi_{\mathbb{Q}}(t)\overline{\phi_{\mathbb{Q}}(t)} \\
&=\mathbb{E}[e^{itX}e^{-itX'}]-\mathbb{E}[e^{itX}e^{-itY}]
-\mathbb{E}[e^{itY}e^{-itX}] + \mathbb{E}[e^{itY}e^{-itY'}]\\
&=\mathbb{E}[e^{it(X-X')}-e^{it(Y-X)}-e^{it(X-Y)}+e^{it(Y-Y')} \\
&=\mathbb{E}[cos(t(X-X')) + isin(t(X-X'))-cos(t(Y-X))-isin(t(Y-X)) -cos(t(X-Y)) \\
&-isin(t(X-Y)) + cos(t(Y-Y'))+isin(t(Y-Y')) \\
&\text{sin(X) = -sin(-X), cos(X) = cos(-X), sin (x- y)=sin(x)cos(y)- cos(x)sin(y)}\\
&=\mathbb{E}[cos(t(X-X')) - 2cos(t(Y-X)) + cos(t(Y-Y')) \\
& + isin(t(X-X')) + isin(t(Y-Y'))\\
&= \mathbb{E}[2(1-cos(t(Y-X)))-(1-cos(t(X-X')))-(1-cos(t(y-Y')))]
\end{split}
\end{equation}
Applying Fubini and the previous lemma:
\begin{equation}{}
\begin{split}
\int_{\mathbb{R}^{d}}\frac{\abs{\phi_{\mathbb{P}}(t)-\phi_{\mathbb{Q}}(t)}^{2}}{\norm{t}^{d+1}_{d}}
&=\int_{\mathbb{R}^{d}}\frac{\mathbb{E}[2(1-cos(t(Y-X)))-(1-cos(t(X-X')))]}{\norm{t}^{d+1}_{d}} dt \\
&=2\mathbb{E}[\int_{\mathbb{R}^{d}}\frac{1-cos(t(Y-X))}{\norm{t}^{d+1}_{d}}] - \mathbb{E}[\int_{\mathbb{R}^{d}}\frac{1-cos(t(X-X'))}{\norm{t}^{d+1}_{d}} - - \mathbb{E}[\int_{\mathbb{R}^{d}}\frac{1-cos(t(Y-Y'))}{\norm{t}^{d+1}_{d}} \\
&=2\mathbb{E}[c_{d}\norm{Y-X}] - \mathbb{E}[c_{d}\norm{X-X'}] -\mathbb{E}[c_{d}\norm{Y-Y'}] \\
&= c_{d}(2\mathbb{E}[\norm{Y-X}] - \mathbb{E}[\norm{X-X'}] -\mathbb{E}[\norm{Y-Y'}])\\
&= c_{d}\varepsilon(X,Y)
\end{split}
\end{equation}
\end{proof}
It is easy to see that the energy distance only vanishes when the distributions are equal, since it is equivalent to having equal characteristic functions.
\subsection{Application to an independence test}
In this subsection we will use the knowledge aquired above to develop a new independence test. This new test is called distance covariance (DCOV), it's name comes from the fact that it is a generalization of the classical product-moment covariance.

We will start by defining the independence test. Given the random vectors $X\in\mathbb{R}^{dx},Y\in\mathbb{R}^{dy}$, distributions $\mathbb{P}_{X} and \mathbb{P}_{Y}$ respectively.Let $\phi_{\mathbb{P}_{X}}, \phi_{\mathbb{P}_{Y}}$ denote their characteristic functions and $\phi_{\mathbb{P}_{XY}}$ the characteristic function of the joint distribution. X and Y are independent if and only if 
$\phi_{\mathbb{P}_{X}}\phi_{\mathbb{P}_{Y}} = \phi_{\mathbb{P}_{XY}}$. The covariance energy test is based on measuring a distance between these functions.

First we need to generalize the energy distance expression for random vectors of different dimensions. As defined earlier this expression is obtained from a weighted $mathcal{L}_{2}$-distance, imposing rotation invariance and scale equivariance, the energy distance is:
\vspace{5mm}

$$\varepsilon(X,Y) = \frac{1}{c_{d_{x}}c_{d_{y}}}\int_{\mathbb{R}^{d_{x} + d_{y}}}\frac{\abs{\phi_{\mathbb{P}}(t)-\phi_{\mathbb{Q}}(t)}^{2}}{\norm{t}^{d_{x}+1}_{d_{x}}\norm{s}^{d_{y}+1}_{d_{y}}}dtds$$

Where $c_{d}$ is defined as before.
The distance covariance is defined by replacing $\phi_{\mathbb{P}}$ and $\phi_{\mathbb{Q}}$ in the previous formula with characteristic functions of the joint distribution and the product of the marginals respectively.
\begin{defn}
The distance covatiance, DCOV, between random vectors X and Y, with $\mathbb{E}\norm{X}_{d_{x}} + \mathbb{E}\norm{Y}_{d_{y}} < \infty$, is the nonnegative number $\nu^{2}(X,Y)$ defined by:
\vspace{5mm}
$$
\nu^{2}(X,Y) = \norm{\phi_{\mathbb{P}_{X,Y}}(t,s)-\phi_{\mathbb{P}_{X}}(t)\phi_{\mathbb{P}_{Y}}(s)}^{2}_{w} = \frac{1}{c_{d_{x}}c_{d_{y}}}\int_{\mathbb{R}^{d_{x} + d_{y}}}\frac{\abs{\phi_{\mathbb{P}_{X,Y}}(t,s)-\phi_{\mathbb{P}_{X}}(t)\phi_{\mathbb{P}_{Y}}(s)}^{2}}{\norm{t}^{d_{x}+1}_{d_{x}}\norm{s}^{d_{y}+1}_{d_{y}}} dtds
$$
\end{defn}
\begin{defn}
The distance correlation, DCOR, between random vectors X and Y, ith $\mathbb{E}\norm{X}_{d_{x}} + \mathbb{E}\norm{Y}_{d_{y}} < \infty$, is the nonnegative number $\mathcal{R}(X,Y)$ defined by:

$$
\mathcal{R}(X,Y) = \left \{
	\begin{array}{c} 
		\frac{\nu^{2}(X,Y)}{\sqrt{\nu^{2}(X)\nu^{2}(Y)}} \text{ if } \nu^{2}(X)\nu^{2}(Y) > 0 \\ 
		\\
		0 \text{ if } \nu^{2}(X)\nu^{2}(Y) = 0 
	\end{array}
	\right.  
$$
\end{defn}
The distance covariance, like the energy distance, can be expressed using expectations.
\begin{lem}
Let (X,Y),(X',Y'),(X'',Y'') $\sim \mathbb{P}_{XY}$ be iid copies of (X,Y), it holds that:
\begin{equation}{DCOV}
\begin{split}
\nu^{2}(X,Y)  
&= \mathbb{E}_{XY}\mathbb{E}_{X'Y'}\norm{X-X'}_{d_{x}}\norm{Y-Y'}_{d_{y}} + \mathbb{E}_{X}\mathbb{E}_{X'}\norm{X-X'}_{d_{x}}\mathbb{E}_{Y}\mathbb{E}_{Y'}\norm{Y-Y'}_{d_{y}} \\
&-2\mathbb{E}_{XY}[\mathbb{E}_{X'}\norm{X-X'}_{d_{x}}\mathbb{E}_{Y'}\norm{Y-Y'}_{d_{y}}]
\end{split}
\end{equation} 
This proof is similar to the one of \ref{DcovDemRara}. therefore we will leave it for the interested readers.
\end{lem}

\subsection{Statistics}
Now we will give some estimators for both energy distance and distance covariance. Since we are interested in testing independence we will focus on the DCOV estimator. We will start with an estimator of energy distance, which as it's explained above it's a homogeneity test.
Given the definition of energy distance \ref{energy_distance} now we will define it's statistic as: Given two independent random samples $x = (x_{1},...,x_{n})$ and  $y = (y_{1},...,y_{m})$, the two sample energy statistic corresponding to $\varepsilon(X,Y)$ is:

$$
\varepsilon_{n,m}(x,y) = \frac{2}{nm}\sum_{i=1}^{n}\sum_{j=1}^{m}\norm{x_{i}-y_{j}} - \frac{1}{n^{2}}\sum_{i=1}^{n}\sum_{j=1}^{n}\norm{x_{i}-x_{j}}- \frac{1}{m^{2}}\sum_{i=1}^{m}\sum_{j=1}^{m}\norm{y_{i}-y_{j}}
$$
Finally, an estimator of the distance covariance can be obtained directly from \ref{DCOV}.For a random sample $(x,y) = ((x_{1},y_{1}),...,(x_{n},y_{n}))$ of iid random vectors generated from the joint distribution of $X \in \mathbb{R}^{dx}$ and $Y \in \mathbb{R}^{dy}$, we obtain:
\begin{equation}{}
\begin{split}
\nu^{2}(X,Y)  
&= \frac{1}{n^{2}}\sum_{i,j=1}^{n}\norm{x_{i}-x_{j}}_{d_{x}}\norm{y_{i}-y_{j}}_{d_{y}} + \frac{1}{n^{2}}\sum_{i,j=1}^{n}\norm{x_{i}-x_{j}}_{d_{x}}\frac{1}{n^{2}}\sum_{i,j=1}^{n}\norm{y_{i}-y_{j}}_{d_{y}}\\
& -\frac{2}{n^{3}}\sum_{i=1}^{n}[\sum_{j=1}^{n}\norm{x_{i}-x_{j}}_{d_{x}}\sum_{j=1}^{n}\norm{y_{i}-y_{j}}_{d_{y}}]
\end{split}
\end{equation} 

As we can see this estimate cost is $O(n^{2})$, that's the reason we won't calculate the distance covariance this way, our new approach will go as follows:
First we compute the Euclidean distance matrix of each sample, computing all the pairwise distances between sample observations:

$$(a_{ij}) = (\norm{x_{i}-x_{j}}_{dx}), (b_{ij}) = (\norm{y_{i}-y_{j}}_{dy}).$$

an easy way to compute this matrix is:

$$A_{ij} = a_{ij} + \overline{a_{i \cdot}} - \overline{a_{\cdot j}} + \overline{a}, for i,j = 1,...,n$$

where:

$$\overline{a_{i\cdot}} = \frac{1}{n}\sum_{k=1}^{n}a_{ik},\overline{a_{\cdot j}} = \frac{1}{n}\sum_{k=1}^{n}a_{kj}, \overline{a} =  \frac{1}{n^{2}}\sum_{k=1}^{n}\sum_{l=1}^{n}a_{lk}$$

equivalently for B. In terms of these matrix, the distance covariance $\nu^{2}(x,y)$ is:

$$\nu^{2}(x,y) = \frac{1}{n^{2}}\sum_{i=1}^{n}\sum_{j=1}^{n}A_{ij}B_{ij}$$
Finally the distance correlation is:

$$
\mathcal{R}(X,Y) = \left \{
	\begin{array}{c} 
		\frac{\nu^{2}_{n}(x,y)}{\sqrt{\nu^{2}_{n}(x)\nu^{2}_{n}(y)}} \text{ if } \nu^{2}_{n}(x)\nu^{2}_{n}(y) > 0 \\ 
		\\
		0 \text{ if } \nu^{2}_{n}(x)\nu^{2}_{n}(y) = 0 
	\end{array}
	\right.  
$$
where: 

$$
\nu^{2}_{n}(x) = \nu^{2}_{n}(x,x) = \frac{1}{n^{2}}\sum_{i=1}^{n}\sum_{j=1}^{n}A_{ij}
$$
$$
\nu^{2}_{n}(y) = \nu^{2}_{n}(y,y) = \frac{1}{n^{2}}\sum_{i=1}^{n}\sum_{j=1}^{n}B_{ij}
$$

Now we will proove that this statistics converge almost surely when the random vectors have finite first moments.

\begin{thm}\label{Objective}
if $\mathbb{E}\norm{X} + \mathbb{E}\norm{Y} < \infty$ then

$$
\lim_{n\to\infty} \nu^{2}_{n}(x,y) \xrightarrow{a.s} \nu^{2}(X,Y)
$$
\end{thm}

In order to proove this theorem we will give an alternative definition of the empirical DCOV statistic in order to make an elegant demonstration.

\begin{defn}
Given all the introduction of this section it'd have been natural, but less elementary, to define $\nu_{n}(x,y)$ as $\norm{f_{XY}^{n}(t,s) - f_{X}^{n}(t)f_{Y}^{n}(s)}$ where:

$$
f_{XY}^{n}(t,s) = \frac{1}{n}\sum_{k=1}^{n}exp[i<t,x_{k}> + i<s,y_{k}>] 
$$

is the empirical characteristic function of the sample $((x_{1},y_{1}),...,(x_{n},y_{n}))$ and 

$$
f_{X}^{n}(t) = \frac{1}{n}\sum_{k=1}^{n}exp[i<t,x_{k}>]
$$

$$
f_{Y}^{n}(s) = \frac{1}{n}\sum_{k=1}^{n}exp[i<s,y_{k}>]
$$
are the marginal empirical characteristic functions of the X sample and Y sample, respectively.
\end{defn}

The next theorem shows that the two definitions are equivalent.

\begin{thm}
If (X,Y) is a sample from the joint distribution of (X,Y), then

$$
\nu_{n}^{2}(X,Y) = \norm{f_{XY}^{n}(t,s) - f_{X}^{n}(t)f_{Y}^{n}(s)}^{2}
$$
\end{thm}

\begin{proof}
Lemma \ref{cd} implies that there exist constants $c_{p}$ and $c_{q}$ such that for all $X\in \mathbb{R}^{p}, y\in \mathbb{R}^{q}$.

$$
\int_{\mathbb{R}^{p}}\frac{1- exp[i<t,X>]}{\norm{t}_{p}^{1+p}}dt = c_p\norm{X}_{p}
$$
$$
\int_{\mathbb{R}^{q}}\frac{1- exp[i<s,Y>]}{\norm{s}_{p}^{1+p}}dt = c_q\norm{Y}_{q}\\
$$
$$
\int_{\mathbb{R}^{p}}\int_{\mathbb{R}^{q}}\frac{1- exp[i<t,X>+i<s,Y>]}{\norm{t}_{p}^{1+p}\norm{s}_{p}^{1+p}}dt = c_qc_p\norm{X}_{p}\norm{Y}_{q}
$$
where the integrals are understood in the principal value sense. For simplicity, consider the case p=q=1. The distance between the empirical characteristic functions in the weighted norm involves $\norm{f_{XY}^{n}(t,s)}^{2}$ , $\norm{f_{X}^{n}(t)f_{Y}^{n}(s)}^{2}$ and $\overline{f_{XY}^{n}(t,s)}f_{X}^{n}(t)f_{Y}^{n}(s)$.
Now we will give the result of evaluating this, due to the similarity to previous demostrations.

$\norm{f_{XY}^{n}(t,s)}^{2} = \frac{1}{n^{2}}\sum_{k,l=1}^{n} cos(X_{k}-X_{l})tcos(Y_{k}-Y_{l})s + V_{1}$

where $V_{1}$ represents terms that vanish when the integral $\norm{f_{XY}^{n}(t,s) - f_{X}^{n}(t)f_{Y}^{n}(s)}^{2}$ is evaluated.

$\norm{f_{X}^{n}(t)f_{Y}^{n}(s)}^{2} = \frac{1}{n^{2}}\sum_{k,l=1}^{n} cos(X_{k}-X_{l})t+ \frac{1}{n^{2}}\sum_{k,l=1}^{n}cos(Y_{k}-Y_{l})s + V_{2}$

$\overline{f_{XY}^{n}(t,s)}f_{X}^{n}(t)f_{Y}^{n}(s) = \frac{1}{n^{3}}\sum_{k,l,m=1}^{n} cos(X_{k}-X_{l})tcos(Y_{k}-Y_{l})s + V_{3}$


where $V_{2}$ and $V_{3}$ represent terms that vanish when the integral is evaluated. To evaluate the integral $\norm{f_{XY}^{n}(t,s) - f_{X}^{n}(t)f_{Y}^{n}(s)}^{2}$, apply \ref{cd} and use:

$$
cos(u)cos(v) = 1-(1-cos(u))-(1-cos(v))+(1-cos(u))(1-cos(v))
$$

After cancellation in the numerator of the integrand it remains to evaluate integrals of the type:

\begin{equation}{}
\begin{split}
\int_{\mathbb{R}^{2}}(1-cos(X_{k}-X_{l})t)(1-cos(Y_{k}-Y_{l})s)\frac{dt}{t^{2}}\frac{ds}{s^{2}} &
= \int_{\mathbb{R}}(1-cos(X_{k}-X_{l})t)\frac{dt}{t^{2}}\int_{\mathbb{R}}(1-cos(Y_{k}-Y_{l})s)\frac{ds}{s^{2}} \\
&= c_{1}^{2}\norm{X_{i}-X_{j}}\norm{Y_{i}-Y_{j}}
\end{split}
\end{equation}
where the first equality comes from applying Fubini.

For random vectors $X\in\mathbb{R}^{p}$ and $Y\in\mathbb{R}^{q}$, the same steps are applied.Thus

$$
\norm{f_{XY}^{n}(t,s) - f_{X}^{n}(t)f_{Y}^{n}(s)}^{2} = S_{1} +S_{2}-2S_{3}
$$

Where:

$$
S_{1} =\frac{1}{n^{2}} \sum_{i,j=1}^{n}\norm{x_{i}-x_{j}}_{p}\norm{y_{i}-y_{j}}_{q}
$$
$$
S_{2} =\frac{1}{n^{2}} \sum_{i,j=1}^{n}\norm{x_{i}-x_{j}}_{p}\frac{1}{n^{2}} \sum_{i,j=1}^{n}\norm{x_{i}-x_{j}}_{p}\norm{y_{i}-y_{j}}_{q}
$$
$$
S_{1} =\frac{1}{n^{3}} \sum_{i=1}^{n}\sum_{j,k=1}^{n}\norm{x_{i}-x_{j}}_{p}\norm{y_{i}-y_{k}}_{q}
$$
\end{proof}
Now that we have proven the equality we will proove the theorem \ref{Objective}
\begin{proof}
Define 

$$
\zeta_{n}(t,s) = \frac{1}{n}\sum_{k=1}^{n}e^{i<t,X_{k}>+i<s,Y_{k}>}- \frac{1}{n}\sum_{k=1}^{n}e^{i<t,X_{k}>}\frac{1}{n}\sum_{k=1}^{n}e^{i<s,Y_{k}>}
$$

so that $\nu^{2}_{n} = \norm{\zeta_{n}(t,s)}^{2}$. Then after elementary transformations: $u_{k} = exp(i<t,X_{k}>)-f_{X}(t)$ and $v_{k} = exp(i<s,Y_{k}>)-f_{Y}(s)$.

For each $\theta>0$ define the region:

$$
D(\theta) = \{(t,s):\theta\leq\norm{t}_{p}\leq\frac{1}{\theta}, \theta\leq\norm{s}_{q}\leq\frac{1}{\theta}\}
$$ 
and random variables 

$$
\nu^{2}_{n,\theta} = \int_{D(\theta)}\norm{\zeta_{n}(t,s)}^{2}dw
$$
For any fixed $\theta>0$, the weight function w(t,s) is bounded on $D(\theta)$. Hence $\nu^{2}_{n,\theta}$ is a combination of V-statistics of bounded random variables, therefore by the strong law of large numbers
it follows almost surely.

$$
\lim_{n\to\infty}\nu^{2}_{n,\theta} = \nu^{2}_{\cdot,\theta} = \norm{f_{XY}(t,s) - f_{X}(t)f_{Y}(s)}^{2}dw
$$
Clearly $\nu^{2}_{\cdot,\theta}$ converges to $\nu^{2}$ as $\theta$ tends to zero. Now it remains to prove that almost surely

$$
\lim\sup_{\theta \to 0} \lim\sup_{n \to \infty} \norm{\nu^{2}_{n,\theta}-\nu^{2}_{n}} = 0
$$

For each $\theta>0$

\begin{equation}{asymptotic_inequality}
\begin{split}
\norm{\nu^{2}_{n,\theta} - \nu^{2}_{n}} \leq &
\int_{\norm{t}_{p}\leq\theta}\norm{\zeta(t,s)}^{2}dw + \int_{\norm{t}_{p}>\frac{1}{\theta}}\norm{\zeta(t,s)}^{2}dw \\
& + \int_{\norm{s}_{q}\leq\theta}\norm{\zeta(t,s)}^{2}dw + \int_{\norm{s}_{q}>\frac{1}{\theta}}\norm{\zeta(t,s)}^{2}dw
\end{split}
\end{equation}

For z = $(z_{1},..,z_{p})$ in $\mathbb{R}^{p}$ define the function

$$
G(y) = \int_{\norm{z}<y} \frac{1-cos(z_{1})}{\norm{z}^{1+p}}
$$

Clearly G(y) is bounded by $c_{p}$ and $\lim_{y\to0}G(y) = 0$. Applying the inequality $\norm{x+y}^{2}\leq2\norm{x}^{2}+2\norm{y}^{2}$ and the following inequality.
\begin{prop}
The Cauchy–Schwarz inequality states that for all vectors u and v of an inner product space it is true that

$|\langle \mathbf {u} ,\mathbf {v} \rangle |^{2}\leq \langle \mathbf {u} ,\mathbf {u} \rangle \cdot \langle \mathbf {v} ,\mathbf {v} \rangle $

where $ \langle \cdot ,\cdot \rangle $ is the inner product. By taking the square root of both sides, and referring to the norms of the vectors, the inequality is written as \cite{C-S-B_inequality1}\cite{C-S-B_inequality2}

$ |\langle \mathbf {u} ,\mathbf {v} \rangle |\leq \|\mathbf {u} \|\|\mathbf {v} \|$

If $ u_{1},\ldots ,u_{n}\in \mathbb {C} $ and $ v_{1},\ldots ,v_{n}\in \mathbb {C} $, and the inner product is the standard complex inner product, then the inequality may be restated more explicitly as follows 

$ |u_{1}{\bar {v}}_{1}+\cdots +u_{n}{\bar {v}}_{n}|^{2}\leq (|u_{1}|^{2}+\cdots +|u_{n}|^{2})(|v_{1}|^{2}+\cdots +|v_{n}|^{2})$

\end{prop} 

one can obtain that:
\begin{equation}{}
\begin{split}
\norm{\zeta_{n}(t,s)}^{2}&
\leq 2 \norm{\frac{1}{n}\sum_{k=1}^{n}u_{k}v_{k}}^{2} + 2\norm{\frac{1}{n}\sum_{k=1}^{n}u_{k}\frac{1}{n}\sum_{k=1}^{n}v_{k}}^{2} \\
\leq \frac{4}{n}\sum_{k=1}^{n}\norm{u_{k}}^{2}\frac{1}{n}\sum_{k=1}^{n}\norm{v_{k}}^{2}
\end{split}
\end{equation}

Therefore the first summand in \ref{asymptotic_inequality} satisfies

$$
\int_{\norm{t}_{p}\leq\theta}\norm{\zeta(t,s)}^{2}dw \leq \frac{4}{n}\sum_{k=1}^{n}\int_{\norm{t}_{p}\leq\theta} \frac{\norm{u_{k}}^{2}dt}{c_{p}\norm{t}^{1+p}_{p}}\frac{1}{n}\sum_{k=1}^{n}\int_{\mathbb{R}^{q}}\frac{\norm{v_{k}}^{2}ds}{c_{q}\norm{s}^{1+q}_{q}}
$$

Here $\norm{v_{k}}^{2} = 1 + \norm{f_{Y}(s)}^{2}- exp(i<s,Y_{k}>)\overline{f_{Y}(s)} - exp(-i<s,Y_{k}>)f_{Y}(s)$, thus

$$
\int_{\mathbb{R}^{q}}\frac{\norm{v_{k}}^{2}ds}{c_{q}\norm{s}^{1+q}_{q}} = (2E_{Y}\norm{Y_{k}-Y}- E\norm{Y-Y'}) \leq 2(\norm{Y_{k}}+E\norm{Y})
$$

where the expectation $E_{Y}$ is taken with respect to Y, and Y'$=^{D}Y$ is independent of $Y_{k}$. Further, after a suitable change of variables

\begin{equation}{}
\begin{split}
\int_{\norm{t}_{p}\leq\theta} \frac{\norm{u_{k}}^{2}dt}{c_{p}\norm{t}^{1+p}_{p}} &
= 
\end{split}
\end{equation}
\end{proof}





