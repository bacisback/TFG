Since the 8th century, when Al-Khali(717-786) wrote the \textit{Book of Cryptographic Messages} which contains the first use of permutations and combinations \cite{historia} humans have shown interest on the study of the  likelihood of events. In the eighteenth century with Jacob Bernoulli's \textit{Ars Conjectandi} (posthumous, 1713) \cite{bernoulli} a version of the fundamental law of large numbers was proven, which states that in a large number of trials, the average of the outcomes is likely to be very close to the expected value, probability became one of the main mathematical fields, introducing probability measures.  Probability measures are widely used in hypothesis testing, density estimation, Markov chain and Monte Carlo to give some examples. In this work our main focus will be hypothesis testing, mainly homogeneity testing. 

The goal in homogeneity testing is to accept or reject the null hypothesis $\mathcal{H}_{0}$:$\mathbb{P}=\mathbb{Q}$, versus the alternative hypothesis $\mathcal{H}_{1}$:$\mathbb{P}\neq\mathbb{Q}$, for a class of probability distributions $\mathbb{P}$ and $\mathbb{Q}$. For this purpose we will define a metric $\gamma$ such that testing the null hypothesis is equivalent to testing for $\gamma(\mathbb{P}\mathbb{Q}) = 0$. We are specially interested in testing for independence between random vectors, which is a particular case of homogeneity testing, using $\mathbb{P} = \mathbb{P}_{\mathcal{XY}}$ and $\mathbb{Q} = \mathbb{P}_{\mathcal{X}}\cdot\mathbb{P}_{\mathcal{Y}}$, where $\mathbb{P} =\mathbb{P}_{\mathcal{XY}}$ is the joint distribution and  $\mathbb{Q} = \mathbb{P}_{\mathcal{X}}\cdot\mathbb{P}_{\mathcal{Y}}$ is the product of the marginal distribution.

Measuring the existance of dependence between variables is a classical yet fundamental problem in statistics. Starting with Auguste Bravais and Francis Galton's correlation coefficient defined as a product-moment, and it's relation with linear regresion \textit{Stigler (1989)},  many  techniques  have  been proposed, developed and studied. Nowadays this subject is of fundamental importance in scientific fields such as Physics, Chemistry, Biology, and Economics. A practical application is Principal Component Analysis (PCA), which is a statistical procedure that converts a set of observations of possibly correlated variables into a set of linearly uncorrelated variables called principal components. 

The goal of this project is to study some state of the art dependence measures and compare them, analize for which scenarios one measure may outperform others in order to select for each problem the suited measure. As we go along through the document, we will present three main approches of non-linear dependence measures will be presented: by using kernel independence measures, canonical correlation between random non-linear projections and a characteristic function based test. 
Our aim was for our analisis to be as complete as possible, nevertheless given the time constraints of an undergraduate thesis project, the complete scope of the project wasn't clearly delimited at the beginning. Therefore, for the development process, we followed an agile methodology, where the advance of the project was discussed douring periodic weekly meetings.

Now we will present a brief structure of the work, which is as follows:

In the begining of the work, which is composed of Chapter\cref{CAP:DepMeasures}, we will present a brief introduction of the previous mathematical knowledge needed in order to understand completely the whole document. We will follow presenting an homogeneity test ,MMD, based on mean embeddings of the original variables through non-linear transformations into Hilbert spaces with reproducing kernel ,RKHS, will be introduced this new intuitions will lead us to our first independence test ,HSIC.

Secondly we will study the concept of energy distance and introduce the second independence test ,DCOV. Followed by an study of the equivalence of this tests with MMD.

Our last dependence measure, RDC, an aproximation of the \textit{Hirschfeld-Gebelein-Rényi’s Maximum Correlation
Coefficient} (HGR), defined by Gebelein in 1941, which is based on the canonical correlation between random non-linear projections.

With all the mathematical content explained, we will present software aspect of the project. 
Starting with the design, going in detail about the structure of the software, we will introduce this section with the analysis of requirements of the project which will serve as a connecting flow to transitionate to the design aspects of the project, talking about the relationship between the structures which will be implemented and how they interact with each other.
This will lead us directly to talk about the development aspect, where we will explain how the requirements were managed and problems which may have rosen were solved. Then we will explain a little bit about version control of the project, which tools were used and concluding with the software development methodology wich was followed.

We will conclude with the experiments which were performed in order to compare each of the dependence measures and a final discussion about the results obtained and future work.





