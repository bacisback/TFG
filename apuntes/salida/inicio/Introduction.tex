Since the 8th century, when Al-Khali(717-786) wrote the \textit{Book of Cryptographic Messages} which contains the first use of permutations and combinations \cite{historia} humans have shown interest and studied the likelihood of events. In the eighteenth century with Jacob Bernoulli's \textit{Ars Conjectandi} (posthumous, 1713) \cite{bernoulli} a version of the fundamental law of large numbers was proven, which states that in a large number of trials, the average of the outcomes is likely to be very close to the expected value, probability became one of the main mathematical fields, introducing probability measures.  Probability measures are widely used in hypothesis testing, density estimation, Markov chain and Monte carlo to give some examples. In this work our main focus will be hypothesis testing, mainly homogeneity testing. 

The goal in homogeneity testing is to accept or reject the null hypothesis $\mathcal{H}_{0}$:$\mathbb{P}=\mathbb{Q}$, versus the alternative hypothesis $\mathcal{H}_{1}$:$\mathbb{P}\neq\mathbb{Q}$, for a class of probability distributions $\mathbb{P}$ and $\mathbb{Q}$. For this purpose we will define a metric $\gamma$ such that testing the null hypothesis is equivalent to testing for $\gamma(\mathbb{P}\mathbb{Q}) = 0$. We are specially interested in testing for independence between random vectors, which is a particular case of homogeneity testing, using $\mathbb{P} = \mathbb{P}_{\mathcal{XY}}$ and $\mathbb{Q} = \mathbb{P}_{\mathcal{X}}\cdot\mathbb{P}_{\mathcal{Y}}$. 

Measuring the existance of dependence between variables is a classical yet fundamental problem in statistics. Starting with Auguste Bravais and Francis Galton's correlation coefficient defined as a product-moment, and it's relation with linear regresion [Stigler, 1989],  many  techniques  have  been proposed, developed and studied. Nowadays this subject is of fundamental importance in scientific fields such as physics, chemistry, biology, and economics. A practical application is Principal Component Analysis (PCA), which is a statistical procedure that converts a set of observations of possibly correlated variables into a set of linearly uncorrelated variables called principal components. 

In this work three main approches of non-linear dependence measures will be presented: by using kernel independence measures (HSIC), canonical correlation between random non-linear projections (RDC) and a characteristic function based test (DCOV).

The structure of the work will go as it follows:

In the begining of the work, which is composed of Chapters , an homogeneity test ,MMD, based on mean embeddings of the original variables through non-linear transformations into Hilbert spaces with reproducing kernel ,RKHS, will be introduced this new intuitions will lead us to our first independence test ,HSIC.

Secondly we will study the concept of energy distance and introduce the second independence test ,DCOV. Followed by an study of the equivalence of this tests with MMD.

Finally RDC will be presented, concluding with a comparison of this three tests between them and with other tests. 

\subsection{Reproducing Kernel Hilbert Spaces (RKHS)}



