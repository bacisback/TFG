In the previous section we introduced MMD which was an homogeneity test, now we will use MMD to define an independence test. Originally HSIC was defined as the squared HS norm of the associated cross-covariance operator, but due to this definition being pretty abstract we will manage another definition which will be shown to be equivalent. 
Appendix \ref{dems} Section \ref{P:HSIC} Subsection \ref{P:HSCC} gives more information about the original definition.

After we will determine whether the dependence returned via HSIC is statistically significant by studying an hypothesis test with HSIC as its statistic and testing it empirically.
Finally we will prove the equivalence of the HSIC and MMD applied to $\mathbb{P}_{\mathcal{X}\mathcal{Y}}$ and $\mathbb{P}\mathbb{Q}$.
Most information for this section is taken from \cite{HSICdistribution}, \cite{HSIC_1},\cite{HSIC2},\cite{HSICEquivalenceMMD} and \cite{HSICDegenerate}

We will start defining HSIC in terms of expectations of kernels and head step by step showing that HSIC expressed this way generates an independence test 

If we denote $X,X'\sim \mathbb{P}$ and $Y,Y'\sim \mathbb{Q}$ then:

\begin{equation}[E:HSICEK]{HSIC in terms of expectations of kernels}
HSIC(\mathbb{P}_{\mathcal{X}\mathcal{Y}},\mathcal{H},\mathcal{G}) = \mathbb{E}_{xx'yy'}[k(x,x')l(y,y')] + \mathbb{E}_{xx'}[k(x,x')]\mathbb{E}_{yy'}[l(y,y')] -2\mathbb{E}_{xy}[\mathbb{E}_{x'}[k(x,x')]\mathbb{E}_{y'}[l(y,y')]]
\end{equation}

In the Appendix \ref{dems} Section \ref{P:HSIC} is shown the proof that this definition is equivalent to the squared HS norm of the associated cross-covariance operator

In order for HSIC to generate an independence test we will need that given two random variables $X\sim\mathbb{P}$ and $X\sim\mathbb{Q}$, with joint distribution $\mathbb{P}_{XY}$, and two RKHS's $\mathcal{H}$ and $\mathcal{G}$ with characteristic kernels k and l, then HSIC( $\mathbb{P}_{XY},\mathcal{H},\mathcal{G}$) = 0 if and only if $\mathbb{P}_{XY} = \mathbb{P}\mathbb{Q}$, which is equivalent of X and Y being independent.

We won't prove this proposition because we've already proven in Appendix \ref{dems} Section \ref{P:MMD} Subsection \ref{P:HTMMD}, that MMD defines an homogeneity test, and we will use that HSIC and MMD are equivalent, therefore if for MMD it is true then for HSIC is true as well.

Now we will take a look at this new definition for HSIC and the definition of MMD given in equation \ref{MMDequiv} it's easy to proove that MMD and HSIC are equivalent. (To prove it one just needs to express the kernel $\upsilon$ in terms of the respectives kernels k and l of  $\mathcal{H}$ and $\mathcal{G}$ and unravel the norm in terms of the expectations).

\subsection{Statistics}

In the previous subsection we talked about the theoretical expression for HSIC, but as our objective is to implement this test and work with it, this subsection will show how can we calculate this statistic numerically.
Our objective for this subsection is to prove that the next expression is in fact the empirical HSIC.

\textsf{\textbf{Empirical HSIC}}
\begin{equation}{Empirical HSIC}
HSIC(\mathbb{P}_{\mathcal{X}\mathcal{Y}},\mathcal{H},\mathcal{G}) = (m-1)^{-2}\textbf{tr}KHLH
\end{equation}

where: $H,K,L \in \mathbb{R}^{m \times m}$, $K_{i,j} = k(x_{i},y_{j})$ , $L_{i,j} = l(x_{i},y_{j})$ and $H_{i,j} = \delta_{i,j} - m^{-1}$


In the Appendix \ref{dems} Section \ref{P:HSIC} Subsection \ref{P:EXPHSIC} is shown the proof of this theorem.

Now we will briefly present the real asymptotic distribution of HSIC
\begin{thm}
Under the $\mathcal{H}_{0}$ the U-statistic HSIC corresponding to the V-statistic 
\vspace{5mm}

$$ HSIC(Z) = \frac{1}{m^{4}}\sum_{i,j,q,r \in_{4}^{m}} h_{ijqr}$$ 
is degenerate, meaning $\mathbb{E}h=0$. In this case, HSIC(Z) converges in distribution according to \cite{HSICDegenerate}, section 5.5.2
\vspace{5mm}

$$mHSIC(Z)\rightarrow \sum_{l=1} \lambda_{l}z_{l}^{2}$$
where $z_{l}\sim \mathcal{N}(0,1)$ i.i.d and $\lambda_{l}$ are the solutions to the eigenvalue problem

$$
\lambda_{l}\psi_{l}(z_{j}) = \int h_{ijqr}\psi_{l}(z_{i})dF_{iqr}
$$

where the integral is over the distribution of variables $z_{i},z_{q}$ and $z_{r}$\cite{HSICdistribution}
\end{thm}

As it's easy to see, this expression isn't manageable because we will need to compute the distribution for every distribution we were to test, and if we were to test samples from some distribution then all we could compute would be an estimate of this asymptotic distribution, therefore now we will introduce an easy way of calculating the $1-\alpha$ quantile of the null distribution which is what we need for hypothesis testing.

\paragraph{Approximating the $1-\alpha$ quantile of the null distribution}

A hypothesis test using HSIC(Z)
could be derived from Theorem 3.3 above by computing the $(1 âˆ’ \alpha)$th quantile of the distribution  $\sum_{l=1} \lambda_{l}z_{l}^{2}$,
where consistency of the test (that is, the convergence to zero of the Type II error for $m \rightarrow \infty$) is
guaranteed by the decay as $m^{-1}$ of the variance of HSIC(Z) under $H_{1}$ . The distribution under $H_{0}$
is complex, however: the question then becomes how to accurately approximate its quantiles.

One approach taken by \cite{HSICdistribution} is by using a Gamma distribution, which as we can see in the figure underneath is quite accurate.

Now we will take a look at the next independence test.
